{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1d055f",
   "metadata": {
    "id": "1f1d055f"
   },
   "source": [
    "# 00 Import Libraries\n",
    "We need `requests` for visiting websites, `BeautifulSoup` for scraping websites and `json` for saving the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6149f7b",
   "metadata": {
    "id": "e6149f7b"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece10b5",
   "metadata": {
    "id": "6ece10b5"
   },
   "source": [
    "# 01 Initialize Variables and Beautiful Soup\n",
    "Set the Homepage of the website we want to scrape and initialize lists for those information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8231dc",
   "metadata": {
    "id": "5f8231dc"
   },
   "outputs": [],
   "source": [
    "# HOMEPAGE URL TO SCRAPE\n",
    "homepage = \"https://books.toscrape.com/\"\n",
    "\n",
    "# INITIALIZE BEAUTIFUL SOUP\n",
    "response = requests.get(homepage)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# INITIALIZE LISTS TO STORE INFORMATION\n",
    "books_info = []\n",
    "books_description = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6f3d8",
   "metadata": {
    "id": "79b6f3d8"
   },
   "source": [
    "# 02 Loop To Extract and Clean Information\n",
    "First we need to get all book categories and the links to those categories. We then go to each of those category websites and extract all book titles. We also get the links to each of those titles to get the description of those books. If there is pagination (more then one page per category) we also visit those websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2b09e",
   "metadata": {
    "id": "5ee2b09e"
   },
   "outputs": [],
   "source": [
    "# LOOP FOR CATEGORIES\n",
    "for a in soup.select('.side_categories ul ul li a'):\n",
    "    book_category = a.text.strip()\n",
    "    category_link = a['href'].replace(\"..\", \"\")\n",
    "\n",
    "    # GO TO CATEGORY PAGE AND EXTRACT ALL BOOKS FROM CATEGORY\n",
    "    category_page_url = homepage + category_link\n",
    "\n",
    "    while category_page_url:\n",
    "        response_category = requests.get(category_page_url)\n",
    "        response_category.encoding = 'utf-8'\n",
    "        soup_category = BeautifulSoup(response_category.content, 'html.parser')\n",
    "\n",
    "        # FIND ALL BOOKS INSIDE <li> ELEMENTS\n",
    "        books = soup_category.find_all('li', class_='col-xs-6 col-sm-4 col-md-3 col-lg-3')\n",
    "\n",
    "        # LOOP THRU ALL BOOKS TO EXTRACT INFORMATION\n",
    "        for book in books:\n",
    "\n",
    "            # EXTRACT Title, Rating, Price, Availability\n",
    "            book_title_tag = book.find('h3').find('a')\n",
    "            book_title = book_title_tag.text\n",
    "            book_link = book_title_tag['href'].replace(\"../../../\", \"\")\n",
    "            book_rating = book.find('p', class_='star-rating')['class'][1]\n",
    "            book_price = book.find('p', class_='price_color').text\n",
    "            book_availability = book.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "            # GET description FROM DETAIL PAGE\n",
    "            detail_page_url = homepage + \"catalogue/\" + book_link\n",
    "            response_detail = requests.get(detail_page_url)\n",
    "            soup_detail = BeautifulSoup(response_detail.content, 'html.parser')\n",
    "            \n",
    "            if soup_detail.find('article', class_='product_page').find('p', class_=False):\n",
    "                p_tags = soup_detail.find('article', class_='product_page').find('p', class_=False)\n",
    "                books_description = p_tags.text\n",
    "            else:\n",
    "                books_description = \"No description available\"\n",
    "\n",
    "            # ADD INFORMATION TO LIST\n",
    "            books_info.append({\n",
    "                'book_title': book_title,\n",
    "                'book_category': book_category,\n",
    "                'books_description': books_description,\n",
    "                'book_link': book_link,\n",
    "                'book_rating': book_rating,\n",
    "                'book_price': book_price,\n",
    "                'book_availability': book_availability,\n",
    "            })\n",
    "\n",
    "        # CHECK FOR NEXT-PAGE IN THIS CATEGORY\n",
    "        next_page = soup_category.find('li', class_='next')\n",
    "        if next_page:\n",
    "            url_next = next_page.find('a')['href']\n",
    "            category_page_url = category_page_url.rsplit('/', 1)[0] + '/' + url_next\n",
    "        else:\n",
    "            category_page_url = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54b434",
   "metadata": {
    "id": "5e54b434"
   },
   "source": [
    "# 03 Save Information to a File\n",
    "Save all extracted information in a local JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8904780",
   "metadata": {
    "id": "e8904780"
   },
   "outputs": [],
   "source": [
    "# CONVERT TO JSON\n",
    "json_data = json.dumps(books_info, ensure_ascii=False, indent=4)\n",
    "\n",
    "# SAVE JSON\n",
    "with open('json/books_info.json', 'w') as file:\n",
    "    file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
